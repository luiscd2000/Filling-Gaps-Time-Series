{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoder Model**"
      ],
      "metadata": {
        "id": "zMNDgrsFPq16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kborj2kAblf",
        "outputId": "381530d0-0b57-43d0-efbb-497caf5050cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import random\n",
        "\n",
        "# Set the seed\n",
        "random.seed(19)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/DataThesis/BETN073/working_data.csv\")\n",
        "\n",
        "obj = 0.2"
      ],
      "metadata": {
        "id": "1bQe7TokAytw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "mMCPqNciBJp-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Select columns to normalize\n",
        "columns_to_normalize = ['Concentration']\n",
        "\n",
        "# Fit and transform the selected columns\n",
        "data_normalized = data.copy()  # Make a copy to avoid modifying the original data\n",
        "data_normalized[columns_to_normalize] = scaler.fit_transform(data_normalized[columns_to_normalize])\n",
        "\n",
        "data_normalized = data_normalized[data_normalized['Year'] != 2008]\n",
        "data_normalized = data_normalized.reset_index()\n",
        "data_normalized = data_normalized.drop(columns='index')\n",
        "\n",
        "# Display the normalized data\n",
        "print(data_normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHNa2iKDBtBx",
        "outputId": "df0febfe-d3c4-4bd3-f028-74db22a7ed8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Year  Month  Day  Concentration  DayOfWeek  Weekend\n",
            "0     2009      1    1       0.041503          4        0\n",
            "1     2009      1    2       0.019506          5        0\n",
            "2     2009      1    3       0.027512          6        1\n",
            "3     2009      1    4       0.069484          0        1\n",
            "4     2009      1    5       0.137438          1        0\n",
            "...    ...    ...  ...            ...        ...      ...\n",
            "3983  2019     12   27       0.174413          5        0\n",
            "3984  2019     12   28       0.146468          6        1\n",
            "3985  2019     12   29       0.134440          0        1\n",
            "3986  2019     12   30       0.254002          1        0\n",
            "3987  2019     12   31       0.086472          2        0\n",
            "\n",
            "[3988 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = int(len(data_normalized) * obj)\n",
        "random_indices = np.random.choice(data_normalized.index, missing_values, replace=False)\n",
        "random_indices.sort()\n",
        "data_normalized.loc[random_indices, 'Concentration'] = -1\n",
        "\n",
        "observed_data = data_normalized[data_normalized['Concentration'] != -1]\n",
        "missing_data = data_normalized[data_normalized['Concentration'] == -1]"
      ],
      "metadata": {
        "id": "Cs_o7WeZCAjT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the autoencoder architecture\n",
        "output_dim_concentration = 1  # Concentration feature\n",
        "input_dim_year = 1  # Date feature\n",
        "input_dim_month = 1  # Date feature\n",
        "input_dim_day = 1  # Date feature\n",
        "input_dim_dayweek = 1  # Hour feature\n",
        "input_dim_weekend = 1  # Date feature\n",
        "encoding_dim = 1  # Adjust the size of the encoded representation as needed\n",
        "\n",
        "output_concentration = Input(shape=(output_dim_concentration,))\n",
        "input_year = Input(shape=(input_dim_year,))\n",
        "input_month = Input(shape=(input_dim_month,))\n",
        "input_day = Input(shape=(input_dim_day,))\n",
        "input_dayweek = Input(shape=(input_dim_dayweek,))\n",
        "input_weekend = Input(shape=(input_dim_weekend,))\n",
        "\n",
        "# Concatenate inputs\n",
        "concatenated = Concatenate()([input_year, input_month, input_day, input_dayweek, input_weekend])\n",
        "\n",
        "# Encoder layers\n",
        "encoded = Dense(128, activation='relu')(concatenated)  # First hidden layer\n",
        "encoded = Dense(64, activation='relu')(encoded)      # Second hidden layer\n",
        "encoded = Dense(16, activation='relu')(encoded)      # Third hidden layer\n",
        "encoded = Dense(8, activation='relu')(encoded)      # Fourth hidden layer\n",
        "encoded = Dense(encoding_dim, activation='relu')(encoded)  # Encoding layer #32,16,8\n",
        "\n",
        "# Decoder layers\n",
        "decoded = Dense(8, activation='relu')(encoded)      # First hidden layer in decoder\n",
        "decoded = Dense(16, activation='relu')(decoded)      # Second hidden layer in decoder\n",
        "decoded = Dense(64, activation='relu')(decoded)      # Third hidden layer in decoder\n",
        "decoded = Dense(128, activation='relu')(decoded)      # Fourth hidden layer in decoder\n",
        "decoded = Dense(1, activation='linear')(decoded)\n",
        "\n",
        "autoencoder = Model([input_year, input_month, input_day, input_dayweek, input_weekend], decoded)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "observed_data_float = {\n",
        "    'Concentration': observed_data['Concentration'].astype('float32'),\n",
        "    'Year': observed_data['Year'].astype('float32'),\n",
        "    'Month': observed_data['Month'].astype('float32'),\n",
        "    'Day': observed_data['Day'].astype('float32'),\n",
        "    'DayOfWeek': observed_data['DayOfWeek'].astype('float32'),\n",
        "    'Weekend': observed_data['Weekend'].astype('float32')\n",
        "}\n",
        "\n",
        "missing_data_float = {\n",
        "    'Concentration': missing_data['Concentration'].astype('float32'),\n",
        "    'Year': missing_data['Year'].astype('float32'),\n",
        "    'Month': missing_data['Month'].astype('float32'),\n",
        "    'Day': missing_data['Day'].astype('float32'),\n",
        "    'DayOfWeek': missing_data['DayOfWeek'].astype('float32'),\n",
        "    'Weekend': missing_data['Weekend'].astype('float32')\n",
        "}\n",
        "\n",
        "# Train the autoencoder using only observed data\n",
        "autoencoder.fit([observed_data_float['Year'], observed_data_float['Month'], observed_data_float['Day'], observed_data_float['DayOfWeek'], observed_data_float['Weekend']],\n",
        "                observed_data_float['Concentration'], epochs=10, batch_size=64, shuffle=True, validation_split=0.2)\n",
        "\n",
        "# Predict concentrations for missing dates\n",
        "predicted_concentrations = autoencoder.predict([missing_data_float['Year'], missing_data_float['Month'], missing_data_float['Day'], missing_data_float['DayOfWeek'], missing_data_float['Weekend']])\n",
        "predicted_measurement = scaler.inverse_transform(predicted_concentrations)\n",
        "\n",
        "\n",
        "# Fill in the missing values in the DataFrame with the predicted values\n",
        "missing_data_float['Concentration'] = predicted_measurement.flatten()\n",
        "\n",
        "to_pred = data.loc[random_indices]\n",
        "to_pred_concentration = to_pred['Concentration'].to_numpy()\n",
        "mse_autoencoder = mean_squared_error(to_pred_concentration, missing_data_float['Concentration'])\n",
        "mse_autoencoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d871f8f9-405e-42c9-d128-b8a6663a36ad",
        "id": "GpvGHQqQcvOv"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "40/40 [==============================] - 2s 10ms/step - loss: 2.6284 - val_loss: 0.0426\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0192\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0183\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0185\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0188\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0190\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0193\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0196\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.0153 - val_loss: 0.0191\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0190\n",
            "25/25 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "379.3936310991651"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_autoencoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUjG1G9sdJ9r",
        "outputId": "4ef31878-4d5d-469a-d1f8-39dda520f4c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "379.3936310991651"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}